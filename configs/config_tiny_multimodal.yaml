# Optimal Configuration for Tiny-MultiModal-Larimar to Beat Original Larimar
# Based on Larimar paper findings and multimodal enhancements

# Model Configuration - Optimized for beating Larimar
model:
  # Architecture (Optimal settings based on Larimar paper)
  latent_size: 384 # Optimal from original Larimar experiments
  hidden_size: 768 # BERT-base optimal hidden size
  memory_size: 512 # Increased from original 256 for better episodic storage
  use_memory: true

  # Model Names - Best performing models from literature
  vision_model_name: "facebook/dinov2-base" # State-of-the-art vision encoder
  text_model_name: "bert-base-uncased" # Proven optimal for Larimar
  decoder_model_name: "gpt2-medium" # Better generation than gpt2-base

  # Training Parameters (Optimized based on Larimar paper)
  learning_rate: 1e-4 # Optimal from original Larimar
  weight_decay: 0.01
  warmup_steps: 1000 # Gradual warmup for stability
  max_steps: 50000 # Extended training for better convergence

  # VAE Parameters (Enhanced from original Larimar)
  beta: 1.0 # Full KL weight for better latent learning
  beta_schedule: "linear" # Gradual annealing
  beta_start: 0.0
  beta_end: 1.0
  kl_warmup_steps: 5000 # Longer warmup than original

  # Loss Weights (Balanced for optimal performance)
  reconstruction_strength: 1.0
  memory_strength: 1.0
  kl_weight: 1.0

  # Enhanced Memory Configuration
  memory_warmup_steps: 3000
  direct_writing: true # Faster convergence
  identity_init: true # Better initialization
  observation_noise_std: 0.1 # Regularization

  # Multimodal Fusion (Novel enhancement over original Larimar)
  use_cross_attention: true
  fusion_type: "cross_attention" # Superior to concat/add
  num_attention_heads: 12 # Optimal for BERT-base

  # Generation (Enhanced for better quality)
  max_length: 512
  temperature: 0.9 # Slightly more focused
  top_k: 40 # Reduced for better quality
  top_p: 0.9 # More selective sampling

# Data Configuration - Optimized for beating Larimar
data:
  # Paths
  train_data_path: "data/babylm"
  val_data_path: null
  test_data_path: null

  # Data Type - BabyLM for fair comparison with Larimar
  data_type: "babylm"
  dataset_name: "cc_3M" # Conceptual Captions 3M

  # Processing (Optimal settings)
  max_length: 512 # Full context window
  image_size: 224 # DiNOv2 optimal
  mode: "multimodal"

  # DataLoader (Optimized for training speed)
  batch_size: 16 # Larger batch for better gradients
  num_workers: 4
  pin_memory: true
  persistent_workers: true # Faster data loading

  # Data Augmentation (Novel enhancement)
  text_augmentation: true
  max_samples: null # Use full dataset

# Training Configuration - Optimized for superior performance
trainer:
  # Hardware
  accelerator: "gpu"
  devices: 1
  strategy: "auto"
  precision: "16-mixed" # Mixed precision for efficiency

  # Training (Extended for better convergence)
  max_epochs: 20 # More epochs for thorough learning
  max_steps: 50000 # Extended training
  gradient_clip_val: 1.0
  accumulate_grad_batches: 2 # Effective batch size 32

  # Validation
  val_check_interval: 1000 # More frequent validation
  check_val_every_n_epoch: 1
  limit_val_batches: 100 # Quick validation

  # Optimization (Larimar-optimal settings)
  optimizer: "adamw"
  scheduler: "linear" # Linear decay like original

  # Stability
  fast_dev_run: false
  overfit_batches: 0
  log_every_n_steps: 100
  enable_progress_bar: true

# Logging Configuration - Comprehensive tracking
logging:
  logger: "wandb" # W&B for better tracking
  log_dir: "outputs"

  # Weights & Biases
  wandb_project: "tiny-multimodal-larimar"
  wandb_entity: "babylm-ntust"
  wandb_tags: ["larimar", "multimodal", "babylm", "episodic-memory", "beating-larimar"]

  # Checkpoints
  checkpoint_dir: "outputs"
  save_top_k: 5 # Keep more checkpoints
  save_every_n_epochs: 1
  save_last: true

  # Samples and Monitoring
  save_samples: true
  log_every_n_steps: 100
  log_model_architecture: true

# Callback Configuration
callbacks:
  # Early Stopping
  early_stopping:
    monitor: "val/total_loss"
    patience: 5
    min_delta: 0.001
    mode: "min"

  # Model Checkpoint
  model_checkpoint:
    monitor: "val/total_loss"
    mode: "min"
    save_top_k: 3
    save_last: true
    every_n_epochs: 1

# Experiment Configuration
experiment:
  name: "tiny-multimodal-larimar"
  description: "Tiny multimodal VAE with episodic memory based on Larimar"
  tags: ["multimodal", "vae", "memory", "larimar", "tiny"]

  # Reproducibility
  seed: 42
  deterministic: false

  # Profiling
  profiler: null # "simple", "advanced", "pytorch"

  # Monitoring
  monitor_gpu: true
  monitor_memory: true

# Data Processing Configuration
preprocessing:
  # Text Processing
  text:
    tokenizer_name: "distilbert-base-uncased"
    max_length: 512
    padding: "max_length"
    truncation: true
    lowercase: false

  # Image Processing
  image:
    processor_name: "facebook/dinov2-base"
    image_size: 224
    normalize: true
    augment: false

  # Multimodal
  multimodal:
    alignment: "cross_attention"
    fusion_type: "early" # "early", "late", "hybrid"

# Evaluation Configuration - Comprehensive benchmarks to beat Larimar
evaluation:
  # Core Language Modeling Metrics (Original Larimar benchmarks)
  language_modeling:
    - "perplexity" # Primary metric from Larimar paper
    - "cross_entropy_loss"
    - "bits_per_character"
    - "likelihood"

  # Text Generation Quality (Enhanced evaluation)
  generation_quality:
    - "bleu_1" # 1-gram precision
    - "bleu_2" # 2-gram precision  
    - "bleu_4" # 4-gram precision
    - "rouge_l" # Longest common subsequence
    - "rouge_1" # Unigram recall
    - "rouge_2" # Bigram recall
    - "meteor" # Semantic similarity
    - "bertscore" # Contextual similarity
    - "bleurt" # Learned evaluation metric

  # Multimodal Understanding (Novel capabilities beyond Larimar)
  multimodal_metrics:
    - "vision_text_alignment" # Cross-modal alignment
    - "image_caption_similarity" # Semantic coherence
    - "multimodal_perplexity" # Joint modeling quality
    - "cross_modal_retrieval" # Retrieval accuracy
    - "vision_grounding" # Visual grounding ability

  # Memory System Evaluation (Enhanced episodic memory)
  memory_metrics:
    - "memory_utilization" # How much memory is used
    - "memory_efficiency" # Information per memory slot
    - "episodic_retrieval_accuracy" # Memory recall quality
    - "memory_interference" # Catastrophic forgetting
    - "memory_consolidation" # Long-term retention
    - "attention_entropy" # Memory attention distribution

  # Cognitive Capabilities (Beyond original Larimar)
  cognitive_metrics:
    - "few_shot_learning" # In-context learning ability
    - "compositional_generalization" # Novel combinations
    - "systematic_generalization" # Pattern generalization
    - "transfer_learning" # Cross-domain transfer
    - "continual_learning" # Sequential task learning

  # Computational Efficiency (Practical advantages)
  efficiency_metrics:
    - "inference_speed" # Tokens per second
    - "memory_usage" # Peak GPU memory
    - "training_time" # Time to convergence
    - "parameter_efficiency" # Performance per parameter
    - "flops_efficiency" # Performance per FLOP

  # Generation Configuration
  generation:
    max_length: 128 # Longer generation
    min_length: 10
    num_beams: 5 # Better beam search
    temperature: [0.7, 0.9, 1.0] # Multiple temperatures
    top_k: [25, 40, 50] # Multiple sampling strategies
    top_p: [0.85, 0.9, 0.95]
    do_sample: true
    num_return_sequences: 3 # Multiple outputs
    repetition_penalty: 1.1
    length_penalty: 1.0

  # Benchmark Datasets (Comprehensive evaluation)
  benchmark_datasets:
    # Original Larimar benchmarks
    - "wikitext_103" # Standard language modeling
    - "ptb" # Penn Treebank
    - "lambada" # Long-range dependencies
    
    # Enhanced language understanding
    - "hellaswag" # Commonsense reasoning
    - "piqa" # Physical reasoning
    - "winogrande" # Coreference resolution
    - "arc_easy" # Science QA
    - "arc_challenge" # Advanced science QA
    
    # Multimodal benchmarks
    - "conceptual_captions" # Image captioning
    - "coco_captions" # Standard captioning
    - "flickr30k" # Image-text retrieval
    - "vqa_v2" # Visual question answering
    - "gqa" # Compositional VQA
    
    # Memory and reasoning
    - "babi" # Reasoning tasks
    - "children_book_test" # Reading comprehension
    - "narrativeqa" # Long-form QA

  # Evaluation Schedule
  eval_schedule:
    eval_every_n_steps: 1000
    eval_batch_size: 32
    num_eval_samples: 2000
    save_eval_outputs: true
    compute_expensive_metrics: true

# Memory Configuration - Enhanced beyond original Larimar
memory:
  # Memory Parameters (Optimized from Larimar paper)
  memory_size: 512 # Doubled from original 256
  direct_writing: true # Faster than sequential
  ordering: false # Flexible memory access
  pseudoinverse_approx_step: 3 # Optimal from experiments
  observation_noise_std: 0.1 # Balanced regularization

  # Memory Training (Enhanced)
  memory_strength: 1.0 # Full memory integration
  identity_init: true # Better initialization
  w_logvar_setting: 0 # Single variance (stable)
  deterministic_w: false # Stochastic for exploration

  # Memory Evaluation (Comprehensive analysis)
  memory_analysis: true
  save_memory_states: true # Save for analysis
  visualize_memory: true # Memory visualization
  memory_probing: true # Probe memory contents
  
  # Advanced Memory Features (Novel enhancements)
  memory_consolidation: true # Long-term memory
  memory_interference_analysis: true # Forgetting analysis
  episodic_coherence: true # Episode-level consistency

# Optimization Configuration - Larimar-optimal with enhancements
optimization:
  # Optimizer (Proven optimal from Larimar)
  optimizer:
    name: "adamw"
    lr: 1e-4 # Optimal from original Larimar
    weight_decay: 0.01
    betas: [0.9, 0.999]
    eps: 1e-8

  # Scheduler (Linear like original Larimar)
  scheduler:
    name: "linear"
    warmup_steps: 1000
    max_steps: 50000
    eta_min: 1e-6 # Small final learning rate

  # Gradient (Stable training)
  gradient_clip_val: 1.0
  gradient_clip_algorithm: "norm"

  # Mixed Precision (Efficiency)
  use_amp: true
  amp_level: "O1"

  # Advanced Optimization (Novel enhancements)
  lookahead: false # Can enable for better convergence
  sam: false # Sharpness-Aware Minimization
  gradient_centralization: false

# Hardware Configuration
hardware:
  # GPU
  gpu_ids: [0]
  mixed_precision: false

  # CPU
  num_workers: 4
  pin_memory: true

  # Memory
  max_memory_gb: 16

  # Distributed
  distributed: false
  backend: "nccl"

# Debugging Configuration
debug:
  # Development
  fast_dev_run: false
  overfit_batches: 0
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 1.0

  # Profiling
  profiler: null
  detect_anomaly: false
  track_grad_norm: false

  # Logging
  log_gpu_memory: false
  log_grad_norm: false
  print_model_summary: true

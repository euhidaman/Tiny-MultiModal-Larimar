{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ddd4f91",
   "metadata": {},
   "source": [
    "# Tiny-MultiModal-Larimar Example Usage\n",
    "\n",
    "This notebook demonstrates how to use the Tiny-MultiModal-Larimar model for various multimodal tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320302c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import our model components\n",
    "from src.modules.multimodal_vae import TinyMultiModalVAE\n",
    "from src.modules.data import MultiModalDataset\n",
    "from src.modules.lightning_model import TinyMultiModalLitModel\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f00581a",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('configs/config_tiny_multimodal.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Initialize model\n",
    "model = TinyMultiModalVAE(config['model'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad570a57",
   "metadata": {},
   "source": [
    "## 2. Vision Encoding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2847bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy image (or load a real one)\n",
    "# For demo purposes, we'll create a random image\n",
    "dummy_image = torch.randn(1, 3, 224, 224)  # Batch size 1, RGB, 224x224\n",
    "\n",
    "# Encode the image\n",
    "with torch.no_grad():\n",
    "    vision_features = model.vision_encoder(dummy_image)\n",
    "    \n",
    "print(f\"Vision features shape: {vision_features.shape}\")\n",
    "print(f\"Vision features mean: {vision_features.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12140099",
   "metadata": {},
   "source": [
    "## 3. Text Encoding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bbd8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text input\n",
    "text_input = \"A beautiful sunset over the ocean\"\n",
    "\n",
    "# Tokenize (this is simplified - in practice you'd use proper tokenization)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokens = tokenizer(text_input, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Encode the text\n",
    "with torch.no_grad():\n",
    "    text_features = model.text_encoder(tokens['input_ids'], tokens['attention_mask'])\n",
    "    \n",
    "print(f\"Text features shape: {text_features.shape}\")\n",
    "print(f\"Text features mean: {text_features.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d103791",
   "metadata": {},
   "source": [
    "## 4. Multimodal Fusion Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a272948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuse vision and text features\n",
    "with torch.no_grad():\n",
    "    fused_features = model.multimodal_fusion(vision_features, text_features)\n",
    "    \n",
    "print(f\"Fused features shape: {fused_features.shape}\")\n",
    "print(f\"Fused features mean: {fused_features.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac61d287",
   "metadata": {},
   "source": [
    "## 5. Memory System Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b876d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test memory system\n",
    "batch_size = 1\n",
    "seq_len = 10\n",
    "dummy_input = torch.randn(batch_size, seq_len, model.memory.memory_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    memory_output = model.memory(dummy_input)\n",
    "    \n",
    "print(f\"Memory output shape: {memory_output.shape}\")\n",
    "print(f\"Memory slots used: {model.memory.memory_slots}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0e09d",
   "metadata": {},
   "source": [
    "## 6. End-to-End VAE Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af52b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy multimodal input\n",
    "batch_size = 2\n",
    "images = torch.randn(batch_size, 3, 224, 224)\n",
    "text_ids = torch.randint(0, 1000, (batch_size, 20))  # 20 tokens\n",
    "text_mask = torch.ones(batch_size, 20)\n",
    "\n",
    "# Forward pass through the full model\n",
    "with torch.no_grad():\n",
    "    outputs = model(images, text_ids, text_mask)\n",
    "    \n",
    "print(\"Full model outputs:\")\n",
    "for key, value in outputs.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ede95",
   "metadata": {},
   "source": [
    "## 7. Training Setup Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f5d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how to set up training\n",
    "from src.modules.data import MultiModalDataModule\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Create data module\n",
    "data_module = MultiModalDataModule(\n",
    "    data_dir='data',\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# Create lightning model\n",
    "lit_model = TinyMultiModalLitModel(config)\n",
    "\n",
    "# Create trainer (don't actually train in this example)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    logger=False,\n",
    "    enable_checkpointing=False\n",
    ")\n",
    "\n",
    "print(\"Training setup complete!\")\n",
    "print(f\"Lightning model: {type(lit_model).__name__}\")\n",
    "print(f\"Data module: {type(data_module).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9117bc",
   "metadata": {},
   "source": [
    "## 8. Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0c65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use the model for inference\n",
    "def generate_caption(model, image_tensor, max_length=50):\n",
    "    \"\"\"\n",
    "    Generate a caption for an image using the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode image\n",
    "        vision_features = model.vision_encoder(image_tensor.unsqueeze(0))\n",
    "        \n",
    "        # Create empty text input for generation\n",
    "        generated_ids = []\n",
    "        \n",
    "        # Simple greedy generation (in practice, you'd use more sophisticated decoding)\n",
    "        for _ in range(max_length):\n",
    "            # This is a simplified example - real implementation would be more complex\n",
    "            break\n",
    "            \n",
    "    return \"Generated caption would appear here\"\n",
    "\n",
    "# Example usage\n",
    "example_image = torch.randn(3, 224, 224)\n",
    "caption = generate_caption(model, example_image)\n",
    "print(f\"Generated caption: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24c750f",
   "metadata": {},
   "source": [
    "## 9. Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32961a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model structure\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Model Analysis:\")\n",
    "print(f\"Total parameters: {count_parameters(model):,}\")\n",
    "print(f\"Vision encoder parameters: {count_parameters(model.vision_encoder):,}\")\n",
    "print(f\"Text encoder parameters: {count_parameters(model.text_encoder):,}\")\n",
    "print(f\"Memory parameters: {count_parameters(model.memory):,}\")\n",
    "print(f\"Decoder parameters: {count_parameters(model.decoder):,}\")\n",
    "\n",
    "# Memory usage\n",
    "model_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "print(f\"Model size: {model_size / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b273831",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "This notebook provides a basic overview of the Tiny-MultiModal-Larimar model. For actual usage:\n",
    "\n",
    "1. **Training**: Use `train.py` with your dataset\n",
    "2. **Inference**: Use `inference.py` for generating captions or analyzing images\n",
    "3. **Evaluation**: Use `scripts/evaluate.py` to assess model performance\n",
    "4. **Data Preparation**: Use `scripts/prepare_data.py` to download and prepare datasets\n",
    "\n",
    "The model is designed to be lightweight while maintaining the cognitive and episodic memory aspects of the original Larimar architecture."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
